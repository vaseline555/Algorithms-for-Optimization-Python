{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Optimizations\n",
    "## Chapter 4. Local Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.7.8\n",
      "IPython version      : 7.19.0\n",
      "\n",
      "numpy     : 1.18.4\n",
      "scipy     : 1.6.0\n",
      "tqdm      : 4.51.0\n",
      "matplotlib: 3.3.2\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 4.4.0-176-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,scipy,tqdm,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line search\n",
    "* Page 54 (Algorithm 4.1)\n",
    "  * Method to determine the optimal _step size_ $\\alpha$ when a descent direction $\\mathbf{d}$ is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x[0] * x[1]) + np.exp(x[1] + x[2]) - x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adpated from Ch 03.\n",
    "def bracket_minimum(f, x=0, s=1e-2, k=2.0):\n",
    "    a, ya = x, f(x)\n",
    "    b, yb = a + s, f(a + s)\n",
    "    if yb > ya:\n",
    "        a, b = b, a\n",
    "        ya, yb = yb, ya\n",
    "        s = -s\n",
    "    \n",
    "    while True:\n",
    "        c, yc = b + s, f(b + s)\n",
    "        if yc > yb:\n",
    "            return (a, c) if a < c else (c, a)\n",
    "        a, ya, b, yb = b, yb, c, yc\n",
    "        s *= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(f, x, d):\n",
    "    objective = lambda alpha: f(x + alpha * d)\n",
    "    a, b = bracket_minimum(objective)\n",
    "    alpha = opt.brent(objective, brack=(a, b))\n",
    "    x_opt = x + alpha * d\n",
    "    return alpha, x_opt, f(x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The minimum is at alpha=3.127 with x=[ 1.    -1.127 -0.127] and f(x)=-0.491\n"
     ]
    }
   ],
   "source": [
    "# Page 55\n",
    "alpha, x_opt, f_opt = line_search(f, np.array([1, 2, 3]), np.array([0, -1, -1]))\n",
    "print(f\"[INFO] The minimum is at alpha={alpha:.3f} with x={x_opt.round(4)} and f(x)={f_opt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate line search\n",
    "* Page 56 (Algorithm 4.2)\n",
    "  * Approximate line search using the first Wolfe condition\n",
    "  * First Wolfe condition (sufficient decrease condition; Armijo condition)\n",
    "    * $f(\\mathbf{x}^{(k+1)})\\leq f(\\mathbf{x}^{(k)})+\\beta\\alpha\\nabla_{\\mathbf{d}^{(k)}}{f(\\mathbf{x}^{(k)})}$\n",
    "    * Insufficient to guarantee convergence to a local minimum since a very small step size is still acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, f_grad, x, d, alpha, p=0.5, beta=1e-4):\n",
    "    y, g = f(x), f_grad(x)\n",
    "    while f(x + alpha * d) > y + beta * alpha * np.dot(g, d):\n",
    "        alpha *= p\n",
    "    x_new = x + alpha * d\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Page 57 (Not stated in the book, but for example 4.2)\n",
    "  * Approximate line search using both the first Wolfe condition and the second Wolfe condition\n",
    "  * Second Wolfe condition (curvature condition)\n",
    "    * $\\nabla_{\\mathbf{d}^{(k)}}{f(\\mathbf{x}^{(k+1)})}\\geq\\sigma\\nabla_{\\mathbf{d}^{(k)}}{f(\\mathbf{x}^{(k)})}$\n",
    "    * Directional derivative should be increased in some degree to make a progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search_two(f, f_grad, x, d, alpha, p=0.5, beta=1e-4, sigma=0.9):\n",
    "    y, g = f(x), f_grad(x)\n",
    "    \n",
    "    # Adjust the maximum step size, alpha, until it meets the first Wolfe condition\n",
    "    while f(x + alpha * d) > y + beta * alpha * np.dot(g, d):\n",
    "        alpha *= p\n",
    "        \n",
    "    x_new = x + alpha * d\n",
    "    # Check if the adjusted design point satisfies the second Wolfe condition\n",
    "    while np.dot(d, f_grad(x_new)) < sigma * np.dot(d, f_grad(x)):\n",
    "        alpha *= p\n",
    "        x_new = x + alpha * d\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Page 58 (Example 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_example_4_2(x):\n",
    "    return x[0]**2 + x[0] * x[1] + x[1]**2\n",
    "\n",
    "def f_example_4_2_grad(x):\n",
    "    return np.array([2 * x[0] + x[1], x[0] + 2 * x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracking_line_search(f_example_4_2, f_example_4_2_grad, np.array([1, 2]), np.array([-1, -1]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracking_line_search_two(f_example_4_2, f_example_4_2_grad, np.array([1, 2]), np.array([-1, -1]), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Page 62 (Algorithm 4.3)\n",
    "  * Strong Wolfe condition (curvature condition)\n",
    "    * $\\lvert\\nabla_{\\mathbf{d}^{(k)}}{f(\\mathbf{x}^{(k+1)})}\\rvert\\leq\\sigma\\lvert\\nabla_{\\mathbf{d}^{(k)}}{f(\\mathbf{x}^{(k)})}\\rvert$\n",
    "    * Directional derivative should not become to be too positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_backtracking(f, f_grad, x, d, alpha=1, beta=1e-4, sigma=0.1):\n",
    "    y0, g0, y_prev, alpha_prev = f(x), np.dot(f_grad(x), d), np.nan, 0\n",
    "    alpha_lo, alpha_hi = np.nan, np.nan\n",
    "    \n",
    "    # Bracket phase\n",
    "    while True:\n",
    "        y = f(x + alpha * d)\n",
    "        if y > y0 + beta * alpha * g0 or (np.isnan(y_prev) & (y >= y_prev)):\n",
    "            alpha_lo, alpha_hi = alpha_prev, alpha\n",
    "            break\n",
    "        g = np.dot(d, f_grad(x + alpha * d))\n",
    "        if np.abs(g) <= -sigma * g0:\n",
    "            return alpha\n",
    "        elif g >= 0:\n",
    "            alpha_lo, alpha_hi = alpha, alpha_prev\n",
    "            break\n",
    "        y_prev, alpha_prev, alpha = y, alpha, 2 * alpha\n",
    "        \n",
    "    # Zoom phase\n",
    "    y_lo = f(x + alpha_lo * d)\n",
    "    while True:\n",
    "        alpha = (alpha_lo + alpha_hi) / 2\n",
    "        y = f(x + alpha * d)\n",
    "        if y > y0 + beta * alpha * g0 or y >= y_lo:\n",
    "            alpha_hi = alpha\n",
    "        else:\n",
    "            g = np.dot(d, f_grad(x + alpha * d))\n",
    "            if np.abs(g) <= -sigma * g0:\n",
    "                return alpha\n",
    "            elif g * (alpha_hi - alpha_lo) >= 0:\n",
    "                alpha_hi = alpha_lo\n",
    "            alpha_lo = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strong_backtracking(f_example_4_2, f_example_4_2_grad, np.array([1, 2]), np.array([-1, -1]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
